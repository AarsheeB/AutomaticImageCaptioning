import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import CocoCaptions
from torch.nn.utils.rnn import pack_padded_sequence
from nltk.translate.bleu_score import corpus_bleu
import numpy as np
import matplotlib.pyplot as plt
import nltk

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Preprocessing and data loading
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

train_dataset = CocoCaptions(root='path_to_train_dataset', annFile='annotations_train.json', transform=transform)
test_dataset = CocoCaptions(root='path_to_test_dataset', annFile='annotations_test.json', transform=transform)

# Vocabulary
captions = [caption for _, caps in train_dataset.coco.anns.items() for caption in caps]
words = [word for sent in captions for word in nltk.word_tokenize(sent.lower())]
word_freq = nltk.FreqDist(words)

vocab = [word for word, freq in word_freq.items() if freq > 5]
vocab.insert(0, '<PAD>')
vocab.insert(1, '<SOS>')
vocab.insert(2, '<EOS>')
vocab.insert(3, '<UNK>')

word_to_idx = {word: idx for idx, word in enumerate(vocab)}
idx_to_word = {idx: word for word, idx in word_to_idx.items()}

# Model
class EncoderCNN(nn.Module):
    def __init__(self, embed_size):
        super(EncoderCNN, self).__init__()
        resnet = models.resnet50(pretrained=True)
        modules = list(resnet.children())[:-1]
        self.resnet = nn.Sequential(*modules)
        self.fc = nn.Linear(resnet.fc.in_features, embed_size)
        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)

    def forward(self, images):
        with torch.no_grad():
            features = self.resnet(images)
        features = features.reshape(features.size(0), -1)
        features = self.fc(features)
        features = self.bn(features)
        return features

class DecoderRNN(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):
        super(DecoderRNN, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, features, captions):
        embeddings = self.embed(captions)
        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)
        hiddens, _ = self.lstm(embeddings)
        outputs = self.fc(hiddens)
        return outputs

# Training
def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):
    encoder.train()
    decoder.train()
    total_loss = 0

    for i, (images, captions, lengths) in enumerate(train_loader):
        images = images.to(device)
        captions = captions.to(device)
        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]

        encoder_optimizer.zero_grad()
        decoder_optimizer.zero_grad()

        features = encoder(images)
        outputs = decoder(features, captions)

        loss = criterion(outputs.reshape(-1, outputs.shape[2]), targets)
        loss.backward()

        encoder_optimizer.step()
        decoder_optimizer.step()

        total_loss += loss.item()

        if (i + 1) % 100 == 0:
            print(f'Epoch [{epoch}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')

    return total_loss / len(train_loader)

# Evaluation
def evaluate(encoder, decoder, test_loader, criterion):
    encoder.eval()
    decoder.eval()
    references = []
    hypotheses = []

    with torch.no_grad():
        for images, captions, lengths in test_loader:
            images = images.to(device)
            captions = captions.to(device)

            features = encoder(images)
            outputs = decoder(features, captions)

            sampled_ids = outputs.argmax(2)
            sampled_ids = sampled_ids.cpu().numpy()

            targets = captions.cpu().numpy()

            for i in range(sampled_ids.shape[0]):
                references.append([idx_to_word[idx] for idx in targets[i] if idx != 0])
                hypotheses.append([idx_to_word[idx] for idx in sampled_ids[i] if idx != 0])

    bleu_score = corpus_bleu(references, hypotheses)
    return bleu_score

# Hyperparameters
embed_size = 256
hidden_size = 512
num_layers = 1
learning_rate = 0.001
num_epochs = 10
batch_size = 64

# Model, criterion, optimizer
encoder = EncoderCNN(embed_size).to(device)
decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)
criterion = nn.CrossEntropyLoss()
encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)
decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)

# Data loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# Training loop
for epoch in range(num_epochs):
    train_loss = train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch)
    print(f'Epoch [{epoch}/{num_epochs}], Train Loss: {train_loss:.4f}')

    if (epoch + 1) % 5 == 0:
        bleu_score = evaluate(encoder, decoder, test_loader, criterion)
        print(f'BLEU-4 Score after epoch {epoch + 1}: {bleu_score:.4f}')
